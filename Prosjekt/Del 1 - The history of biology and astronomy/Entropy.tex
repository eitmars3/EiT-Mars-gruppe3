\section*{Physics crash course}

\subsection*{Entropy}
Entropy is in general a measurement of disorder in a system.
In thermodynamics, entropy is a measure of the number of specific ways in which a thermodynamic system way be arranged.
The entropy can never decrease in an isolated system, according to the second law of thermodynamics.
Eventually it will reach thermodynamic equilibrium, the configuration with maximum entropy. \cite{Wiki-entropy]


One can also think of entropy as the probability that a system will be in a certain configuration.
If you have box with 10 balls bouncing around, it is less likely that at any given time they will all be in the same corner.
Therefore the configuration where all the balls are close together in a corner has low entropy.


In thermodynamics the balls can be an analogy for gas particles in the air.
They will be bouncing around from each other, walls, floors and other objects.
The average speed of the particles in the air is proportional to what we call temperature.


Because the entropy in an isolated, or closed, system will always increase it needs a supply of for example energy to keep the entropy low.
This is one of the more intricate requirements for life.
A living organism typically uses metabolism to collect energy, which is transported to the cells of the organism to keep the entropy low and the chemical processes going.

\begin{thebibliography}{99}
	\bibitem{Wiki-entropy}
		Wikipedia,
		\emph{Entropy},
		March 11th 2015
	\bibitem{}
\end{thebibliography}