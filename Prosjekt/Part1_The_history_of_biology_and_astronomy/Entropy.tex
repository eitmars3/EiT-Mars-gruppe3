\subsection*{Entropy}
Imagine a room full of balls bouncing off the walls and each other.
Your image is probably a chaotic mess of balls traveling in all directions and speeds.
The order, or disorder, of the balls is what physicists call entropy.

Entropy is derived from thermodynamics, and it explains some of the most fundamental laws of our universe.

To understand what entropy really means, imagine all of the bouncing balls in your room being in the same corner at the same time.
This must be a very unlikely occurrence.
There are few ways to place all of the balls together in one corner, but lots of ways to spread them all around the room.
When all the balls are in the corner, the room has a low entropy.
If the balls are all over the place, the entropy is high.


The second law of thermodynamics says that the entropy in a closed system never decrease.
In other words; no matter what happens in a closed system, for example a room with no interaction with the outside world, there will be more and more chaos over time.
The system, or room, will eventually reach what we call a thermodynamic equilibrium.
If you want to keep the order in you room, you will need to supply it with energy.
That is partially why humans needs to eat.
Our cells use energy in chemical reactions, and we need to keep the entropy low in order for these chemical reactions to continue.
That is living beings has to be able to work against an increase in entropy, and also the reason this is one of the requirements we use to define life as we know it.

\begin{thebibliography}{99}
	\bibitem{Wiki-entropy}
		Wikipedia,
		\emph{Entropy},
		March 11th 2015
	\bibitem{}
\end{thebibliography}